{
    "layerSizes": [784, 128, 64, 10],
    "activationType": "ReLU",
    "outputActivationType": "Softmax",
    "costType": "CrossEntropy",
    "initialLearningRate": 0.05,
    "learnRateDecay": 0.075,
    "minibatchSize": 32,
    "momentum": 0.9,
    "regularization": 0.1,
    "epochs": 10,
    "trainTestSplit": 0.8
}